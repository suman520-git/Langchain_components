{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58b06684",
   "metadata": {},
   "source": [
    "### Key Features (Ollama):\n",
    "Run models locally – like LLaMA, Mistral, Gemma, Phi, Dolphin (no internet needed).\n",
    "\n",
    "Uses your GPU – if your system has one, it runs faster.\n",
    "\n",
    "Easy commands – works like Docker, very simple for developers.\n",
    "\n",
    "Works with Python – you can use it through API in your Python code.\n",
    "\n",
    "Private & secure – your data stays on your computer, not sent to any server.\n",
    "\n",
    "### How Ollama Works (Internals):\n",
    "You pull a model – Ollama downloads it from its server.\n",
    "\n",
    "The model is saved on your system – it won’t download again.\n",
    "\n",
    "You send a prompt from terminal or Python.\n",
    "\n",
    "Ollama runs the model on your machine and generates a reply.\n",
    "\n",
    "You get the answer via command line or API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b07850",
   "metadata": {},
   "source": [
    "### Install the Ollama: https://ollama.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a861e2bb",
   "metadata": {},
   "source": [
    "| Command                                   | Description                                                     |\n",
    "| ----------------------------------------- | --------------------------------------------------------------- |\n",
    "| `ollama run <model>`                      | Run a model interactively in the terminal                       |\n",
    "| `ollama pull <model>`                     | Download a model (like `llama3`, `mistral`, etc.)               |\n",
    "| `ollama list`                             | Show all models currently installed on your system              |\n",
    "| `ollama show <model>`                     | Show detailed info (parameters, license, modelfile) for a model |\n",
    "| `ollama create <model>`                   | Create a custom model from a Modelfile                          |\n",
    "| `ollama push <model>`                     | Push a custom model to your ollama library (cloud)              |\n",
    "| `ollama serve`                            | Start the local API server                                      |\n",
    "| `ollama run <model> --format json`        | Get responses in JSON format                                    |\n",
    "| `ollama run <model> --system \"<message>\"` | Set a system prompt                                             |\n",
    "| `ollama run <model> --template <file>`    | Use a custom prompt template                                    |\n",
    "| `ollama rm <model>`                       | Remove a model from your local machine                          |\n",
    "| `ollama cp <source> <target>`             | Copy/rename a model                                             |\n",
    "| `ollama pull <model>:<tag>`               | Pull a specific version (e.g., `llama3:latest`)                 |\n",
    "| `ollama run <model> --timeout <seconds>`  | Set timeout for the generation                                  |\n",
    "| `ollama pull llama3 --verbose`              | See detailed download info       |\n",
    "| `ollama run <model> < prompt.txt`           | Feed input from a file           |\n",
    "| `ollama run <model> --context <file>`       | Supply a saved context           |\n",
    "| `ollama run <model> --temperature 0.7`      | Control randomness               |\n",
    "| `ollama run <model> --top-k 50 --top-p 0.9` | Sampling strategies              |\n",
    "| `ollama run <model> --num-predict 100`      | Limit number of tokens generated |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0da628f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A thought-provoking question!\\n\\nSo, you\\'re asking me to think step by step about what LangChain is... Alright, let\\'s break it down!\\n\\nLangChain seems to be related to language processing, which makes sense given the \"lang\" prefix. Could LangChain be a chain of linguistic concepts or models? Perhaps it\\'s a tool that connects different natural language processing (NLP) techniques to create a robust system for text analysis?\\n\\nOr maybe LangChain refers to a specific type of neural network architecture designed for language-related tasks, like machine translation, sentiment analysis, or text generation?\\n\\nYour turn! Would you like me to explore more ideas or clarify your thinking?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = OllamaLLM(model=\"llama3:latest\")\n",
    "#model = OllamaLLM(model=\"deepseek-r1\")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "chain.invoke({\"question\": \"What is LangChain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ef50a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "port requests\n",
    "response = requests.post(\"http://localhost:11434/api/generate\", json={\n",
    "    \"model\": \"llama3\",\n",
    "    \"prompt\": \"What is Ollama?\",\n",
    "    \"stream\": False\n",
    "})im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5215acd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A great question!\n",
      "\n",
      "Ollama is a popular online platform that allows users to create and share interactive, multimedia-rich stories. The website and mobile app enable anyone to become an author, producer, or editor of a story by adding text, images, videos, audio files, and other media elements to a narrative.\n",
      "\n",
      "With Ollama, you can:\n",
      "\n",
      "1. Create your own story from scratch or start with a template.\n",
      "2. Add various media formats like text, photos, videos, audio clips, and even PDFs.\n",
      "3. Arrange the content in a non-linear fashion using a visual timeline.\n",
      "4. Share your finished story with others through social media, email, or direct links.\n",
      "\n",
      "Ollama's innovative storytelling approach has gained significant popularity among creatives, educators, and businesses seeking to engage audiences more effectively. The platform is free to use, making it an excellent tool for anyone looking to experiment with new forms of digital storytelling.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(response.json()[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde44677",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c0b2f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "import requests\n",
    "app = FastAPI()\n",
    "@app.get(\"/chat\")\n",
    "def chat(prompt: str):\n",
    "    r = requests.post(\"http://localhost:11434/api/generate\", json={\n",
    "        \"model\": \"llama3\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    })\n",
    "    return {\"response\": r.json()[\"response\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47126f52",
   "metadata": {},
   "source": [
    "| **Feature**            | **Detail**                               |\n",
    "| ---------------------- | ---------------------------------------- |\n",
    "| **What is Ollama**     | Local LLM runtime                        |\n",
    "| **Installation**       | One-line shell script                    |\n",
    "| **Model usage**        | `ollama run <model>`                     |\n",
    "| **Python integration** | Local REST API                           |\n",
    "| **Ideal for**          | Offline chatbots, privacy-sensitive apps |\n",
    "| **Customization**      | Modelfile-based LLM tweaking             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fc4f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7f08b25",
   "metadata": {},
   "source": [
    "| **Platform**       | **Step**                              | **Command**                         | **Description**                              |                                   |\n",
    "| ------------------ | ------------------------------------- | ----------------------------------- | -------------------------------------------- | --------------------------------- |\n",
    "| **Windows**     | All listening ports with PID          | \\`netstat -aon                      | findstr LISTENING\\`                          | Show all listening ports and PIDs |\n",
    "|                    | All ports with process info           | \\`netstat -aon                      | more\\`                                       | Show all active ports             |\n",
    "|                    | Match PID to process name             | `tasklist /FI \"PID eq 12345\"`       | Get process name using PID                   |                                   |\n",
    "| **Linux/macOS** | All listening ports with process info | `sudo lsof -i -P -n \\| grep LISTEN` | Shows port, protocol, and process info       |                                   |\n",
    "|                    | All TCP/UDP sockets with process      | `sudo netstat -tulnp`               | Traditional method                           |                                   |\n",
    "|                    | Fast socket scan                      | `sudo ss -tulwn`                    | Faster than netstat                          |                                   |\n",
    "| **Python**      | Script to list all listening ports    | See code block below                | Use `psutil` to list occupied ports and PIDs |                                   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3bf1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "connections = psutil.net_connections()\n",
    "for conn in connections:\n",
    "    if conn.status == \"LISTEN\":\n",
    "        print(f\"Port: {conn.laddr.port}, PID: {conn.pid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5c217e",
   "metadata": {},
   "source": [
    "- tasklist | findstr ollama\n",
    "- netstat -ano | findstr :11434\n",
    "- taskkill /PID <PID_NUMBER> /F\n",
    "- taskkill /PID 12345 /F\n",
    "- ollama serve\n",
    "- OLLAMA_PORT=11435 ollama serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e626004",
   "metadata": {},
   "source": [
    "| **Step**         | **Description**                          | **Command**                           |                  |\n",
    "| ---------------- | ---------------------------------------- | ------------------------------------- | ---------------- |\n",
    "| Step 1        | Check if Ollama is already running       | \\`tasklist                            | findstr ollama\\` |\n",
    "| Step 2 (a)    | Find which process is using port `11434` | \\`netstat -ano                        | findstr :11434\\` |\n",
    "| Step 2 (b)    | Kill that process using its PID          | `taskkill /PID <PID_NUMBER> /F`       |                  |\n",
    "| Example       | Example of killing a process             | `taskkill /PID 12345 /F`              |                  |\n",
    "| Step 3        | Restart Ollama server on default port    | `ollama serve`                        |                  |\n",
    "| Optional Step | Run Ollama server on a different port    | `OLLAMA_PORT=11435 ollama serve`      |                  |\n",
    "| Optional Step | Access new port in code/API              | `http://localhost:11435/api/generate` |                  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a07c1d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
